{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model # for model\n",
    "from sklearn.datasets import load_iris # the iris dataset \n",
    "\n",
    "import numpy\n",
    "numpy.set_printoptions(suppress=True) # disable scientific notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris() # load iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [  9.84028024   2.21683511 -12.05711535]  Coefficients: [[-0.41874027  0.96699274 -2.52102832 -1.08416599]\n",
      " [ 0.53123044 -0.31473365 -0.20002395 -0.94866082]\n",
      " [-0.11249017 -0.65225909  2.72105226  2.03282681]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = linear_model.LogisticRegression() # make logistic regression model \n",
    "model.fit(iris.data, iris.target) # fit model to dataset \n",
    "\n",
    "print('Intercept: {0}  Coefficients: {1}'.format(model.intercept_, model.coef_)) # print coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions: [0 1 2]\n",
      "Probabilities:\n",
      "[[0.98180291 0.01819708 0.00000001]\n",
      " [0.00211747 0.87434557 0.12353696]\n",
      " [0.00000089 0.00392306 0.99607606]]\n"
     ]
    }
   ],
   "source": [
    "start_class_two = list(iris.target).index(1)\n",
    "start_class_three = list(iris.target).index(2)\n",
    "# Use the first input from each class\n",
    "inputs = [iris.data[0], iris.data[start_class_two], iris.data[start_class_three]]\n",
    "\n",
    "print('Class predictions: {0}'.format(model.predict(inputs))) # predict which class, should be [0, 1, 2]\n",
    "print('Probabilities:\\n{0}'.format(model.predict_proba(inputs))) # get probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions: [0 1 2]\n",
      "Probabilities:\n",
      "[[0.92831857 0.07157087 0.00011056]\n",
      " [0.00559652 0.62519423 0.36920925]\n",
      " [0.00001365 0.03313084 0.96685551]]\n"
     ]
    }
   ],
   "source": [
    "# use only two features to train second logistic regression model, the first and fourth column \n",
    "x1_feature = 0\n",
    "x2_feature = 3\n",
    "\n",
    "partial_data = iris.data[:,[x1_feature, x2_feature]] # get the first and fourth clumn \n",
    "\n",
    "partial_model = linear_model.LogisticRegression() # make model \n",
    "partial_model.fit(partial_data, iris.target) # fit model \n",
    "\n",
    "partial_inputs = [partial_data[0], partial_data[start_class_two], partial_data[start_class_three]] # make new inputs with only two features each\n",
    "\n",
    "print('Class predictions: {0}'.format(partial_model.predict(partial_inputs))) # predict which class\n",
    "print('Probabilities:\\n{0}'.format(partial_model.predict_proba(partial_inputs))) # get probability of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "\n",
    "Answer the following questions. You can also use the graph below, if seeing the data visually helps you understand the data.\n",
    "1. In the above cell, the expected class predictions should be [0, 1, 2], because the first datapoint of each class was used. If the model was not giving the expected output, some reasons could be that the data values chosen to test were outliers, or that logistic regression does not work well predicting the data. \n",
    "2. How do the probabilities output by the above cell relate to the class predictions? Why do you think the model might be more or less confident in its predictions?\n",
    "3. Looking at the intercept and coefficient output further above, if a coefficient is negative, what has the model learned about this feature? In other words, if you took a datapoint and you increased the value of a feature that has a negative coefficient, what would you expect to happen to the probabilities the model gives this datapoint?\n",
    "4. Do these two features allow you to predict the iris type well? How do you know? Explain using both the text output in the cells above and the graph below.\n",
    "5. Using all the different feature pair combinations, the best pair was petal length and petal width. I know this because I calculated for each combination how confident the model was for the expected outputs. This finding actually aligns with what I found about the iris dataset with a decision tree: in the case of the decision tree, most nodes separated the data based on petal length and petal width, i.e. that they were the best predictors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for sepal length (cm) & sepal width (cm):\n",
      "[[0.92347315 0.0585081  0.01801875]\n",
      " [0.00176572 0.1981595  0.80007478]\n",
      " [0.05009604 0.37235578 0.57754818]]\n",
      "Probabilities for sepal length (cm) & petal length (cm):\n",
      "[[0.97521958 0.02478036 0.00000005]\n",
      " [0.00105972 0.7765676  0.22237268]\n",
      " [0.00000087 0.01201376 0.98798537]]\n",
      "Probabilities for sepal length (cm) & petal width (cm):\n",
      "[[0.92831857 0.07157087 0.00011056]\n",
      " [0.00559652 0.62519423 0.36920925]\n",
      " [0.00001365 0.03313084 0.96685551]]\n",
      "Probabilities for sepal width (cm) & petal length (cm):\n",
      "[[0.98200697 0.01799299 0.00000004]\n",
      " [0.00633308 0.66738949 0.32627743]\n",
      " [0.0000065  0.01584795 0.98414555]]\n",
      "Probabilities for sepal width (cm) & petal width (cm):\n",
      "[[0.95767161 0.04223211 0.00009628]\n",
      " [0.10787733 0.65224682 0.23987585]\n",
      " [0.00009767 0.02361994 0.97628239]]\n",
      "Probabilities for petal length (cm) & petal width (cm):\n",
      "[[0.97983058 0.02016939 0.00000003]\n",
      " [0.0024148  0.77883567 0.21874952]\n",
      " [0.00000027 0.00463449 0.99536524]]\n",
      "Best pair: petal length (cm) & petal width (cm), with score: 0.9180104999500484\n"
     ]
    }
   ],
   "source": [
    "feature_pairs = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]\n",
    "feature_pair_probabilities = []\n",
    "\n",
    "# generate model and probabilities for each feature pair \n",
    "for feature_pair in feature_pairs:\n",
    "    feature_pair_data = iris.data[:,feature_pair] # get data of feature pair \n",
    "    feature_pair_model = linear_model.LogisticRegression() # make model \n",
    "    feature_pair_model.fit(feature_pair_data, iris.target) # fit model to feature pair \n",
    "    feature_pair_inputs = [feature_pair_data[0], feature_pair_data[start_class_two], feature_pair_data[start_class_three]] # make new inputs with only the feature pair\n",
    "    feature_pair_probabilities.append(feature_pair_model.predict_proba(feature_pair_inputs)) # push class probabilities for specific feature pair to array\n",
    "    \n",
    "best_pair_score = 0 # scale from 0-1, how well feature pair performed based on how close it was to expected outputs \n",
    "best_pair = [] # feature pair with best score \n",
    "index = 0 # for indexing \n",
    "\n",
    "for feature_pair_probability in feature_pair_probabilities:\n",
    "    # print probabilities for feature pair \n",
    "    print('Probabilities for {} & {}:\\n{}'.format(iris.feature_names[feature_pairs[index][0]], iris.feature_names[feature_pairs[index][1]], feature_pair_probability))\n",
    "    # calculate score\n",
    "    feature_pair_score = ((feature_pair_probability[0][0]/1)+(feature_pair_probability[1][1]/1)+(feature_pair_probability[2][2]/1))/3\n",
    "    # if it's better than current best feature pair score, update it \n",
    "    if (feature_pair_score > best_pair_score):\n",
    "        best_pair_score = feature_pair_score\n",
    "        best_pair = feature_pairs[index]\n",
    "    # index \n",
    "    index += 1\n",
    "    \n",
    "# print info on the best feature pair \n",
    "print('Best pair: {} & {}, with score: {}'.format(iris.feature_names[best_pair[0]], iris.feature_names[best_pair[1]], best_pair_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #2 - Advanced Difficulty\n",
    "\n",
    "The plot above is only showing the data, and not anything about what the model learned. Come up with some ideas for how to show the model fit and implement one of them in code. Remember, we are here to help if you are not sure how to write the code for your ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
